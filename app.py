import streamlit as st
import pandas as pd
import numpy as np
import re
import requests
import unicodedata
from io import BytesIO
from openpyxl import load_workbook

# === T·∫£i d·ªØ li·ªáu m·∫∑c ƒë·ªãnh t·ª´ GitHub (file2, file3, file4) ===
@st.cache_data
def load_default_data():
    urls = {
        'file2': "https://raw.githubusercontent.com/phamtai1211/Thau_3PPharma/main/file2.xlsx",
        'file3': "https://raw.githubusercontent.com/phamtai1211/Thau_3PPharma/main/file3.xlsx",
        'file4': "https://raw.githubusercontent.com/phamtai1211/Thau_3PPharma/main/nhom_dieu_tri.xlsx"
    }
    data = {}
    for key, url in urls.items():
        resp = requests.get(url)
        resp.raise_for_status()
        data[key] = pd.read_excel(BytesIO(resp.content), engine='openpyxl')
    return data['file2'], data['file3'], data['file4']

file2, file3, file4 = load_default_data()

# === H√†m chu·∫©n h√≥a vƒÉn b·∫£n ===
def remove_diacritics(s: str) -> str:
    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')

def normalize_text(s: str) -> str:
    s = str(s)
    s = remove_diacritics(s).lower()
    return re.sub(r'\s+', '', s)

# === H√†m chu·∫©n h√≥a ho·∫°t ch·∫•t, h√†m l∆∞·ª£ng, nh√≥m ===
def normalize_active(name: str) -> str:
    return re.sub(r'\s+', ' ', re.sub(r'\(.*?\)', '', str(name))).strip().lower()

def normalize_concentration(conc: str) -> str:
    s = str(conc).lower().replace(',', '.')
    parts = [p.strip() for p in re.split(r'[;,]', s) if p.strip()]
    parts = [p for p in parts if re.search(r'\d', p)]
    if len(parts) >= 2 and re.search(r'(mg|mcg|g|%)', parts[0]) and 'ml' in parts[-1]:
        return parts[0].replace(' ', '') + '/' + parts[-1].replace(' ', '')
    return ''.join([p.replace(' ', '') for p in parts])

def normalize_group(grp: str) -> str:
    return re.sub(r'\D', '', str(grp)).strip()

# === Giao di·ªán Sidebar ===
st.sidebar.title("Ch·ª©c nƒÉng")
option = st.sidebar.radio(
    "Ch·ªçn ch·ª©c nƒÉng",
    [
        "L·ªçc Danh M·ª•c Th·∫ßu",
        "Ph√¢n T√≠ch Danh M·ª•c Th·∫ßu",
        "Ph√¢n T√≠ch Danh M·ª•c Tr√∫ng Th·∫ßu",
        "ƒê·ªÅ Xu·∫•t H∆∞·ªõng Tri·ªÉn Khai"
    ]
)

# === 1. L·ªçc Danh M·ª•c Th·∫ßu ===
if option == "L·ªçc Danh M·ª•c Th·∫ßu":
    st.header("üìÇ L·ªçc Danh M·ª•c Th·∫ßu")
    # Ch·ªçn Mi·ªÅn ‚Üí V√πng ‚Üí T·ªânh ‚Üí B·ªánh vi·ªán/SYT
    df3_sel = file3.copy()
    for col in ['Mi·ªÅn', 'V√πng', 'T·ªânh', 'B·ªánh vi·ªán/SYT']:
        opts = ['(T·∫•t c·∫£)'] + sorted(df3_sel[col].dropna().unique())
        sel = st.selectbox(f"Ch·ªçn {col}", opts)
        if sel != '(T·∫•t c·∫£)':
            df3_sel = df3_sel[df3_sel[col] == sel]
    st.session_state['file3_temp'] = df3_sel

    # T·∫£i file danh m·ª•c m·ªùi th·∫ßu
    uploaded = st.file_uploader("T·∫£i l√™n file Danh M·ª•c M·ªùi Th·∫ßu (.xlsx)", type=['xlsx'])
    if uploaded:
        # Ch·ªçn sheet c√≥ nhi·ªÅu c·ªôt nh·∫•t
        xls = pd.ExcelFile(uploaded, engine='openpyxl')
        sheet = max(
            xls.sheet_names,
            key=lambda s: pd.read_excel(uploaded, sheet_name=s, nrows=5, header=None, engine='openpyxl').shape[1]
        )
        # ƒê·ªçc b·∫±ng pandas, fallback openpyxl
        try:
            raw = pd.read_excel(uploaded, sheet_name=sheet, header=None, engine='openpyxl')
        except Exception:
            wb = load_workbook(uploaded, read_only=True, data_only=True)
            ws = wb[sheet]
            data = [row for row in ws.iter_rows(values_only=True)]
            raw = pd.DataFrame(data)

        # T√¨m header row trong 10 d√≤ng ƒë·∫ßu
        header_idx = None
        scores = []
        for i in range(min(10, len(raw))):
            text = normalize_text(' '.join(raw.iloc[i].fillna('').astype(str).tolist()))
            sc = sum(kw in text for kw in ['tenhoatchat','soluong','nhomthuoc','nongdo'])
            scores.append((i, sc))
            if 'tenhoatchat' in text and 'soluong' in text:
                header_idx = i
                break
        if header_idx is None:
            idx, sc = max(scores, key=lambda x: x[1])
            if sc > 0:
                header_idx = idx
                st.warning(f"T·ª± ƒë·ªông ch·ªçn d√≤ng ti√™u ƒë·ªÅ t·∫°i d√≤ng {idx+1}")
            else:
                st.error("‚ùå Kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c d√≤ng ti√™u ƒë·ªÅ.")
                st.stop()

        # G√°n header v√† l·∫•y ph·∫ßn body
        header = raw.iloc[header_idx].tolist()
        df_body = raw.iloc[header_idx+1:].copy()
        df_body.columns = header
        df_body = df_body.dropna(subset=header, how='all')
        df_body['_orig_idx'] = df_body.index
        df_body.reset_index(drop=True, inplace=True)

        # Chu·∫©n h√≥a t√™n c·ªôt ƒë·∫ßu v√†o
        col_map = {}
        for c in df_body.columns:
            n = normalize_text(c)
            if 'tenhoatchat' in n or 'tenthanhphan' in n:
                col_map[c] = 'T√™n ho·∫°t ch·∫•t'
            elif 'nongdo' in n or 'hamluong' in n:
                col_map[c] = 'N·ªìng ƒë·ªô/h√†m l∆∞·ª£ng'
            elif 'nhom' in n and 'thuoc' in n:
                col_map[c] = 'Nh√≥m thu·ªëc'
            elif 'soluong' in n:
                col_map[c] = 'S·ªë l∆∞·ª£ng'
            elif 'duongdung' in n or 'duong' in n:
                col_map[c] = 'ƒê∆∞·ªùng d√πng'
            elif 'gia' in n:
                col_map[c] = 'Gi√° k·∫ø ho·∫°ch'
        df_body.rename(columns=col_map, inplace=True)

        # Chu·∫©n h√≥a file2 (danh m·ª•c s·∫£n ph·∫©m c√¥ng ty)
        df2 = file2.copy()
        col_map2 = {}
        for c in df2.columns:
            n = normalize_text(c)
            if 'tenhoatchat' in n:
                col_map2[c] = 'T√™n ho·∫°t ch·∫•t'
            elif 'nongdo' in n or 'hamluong' in n:
                col_map2[c] = 'N·ªìng ƒë·ªô/h√†m l∆∞·ª£ng'
            elif 'nhom' in n and 'thuoc' in n:
                col_map2[c] = 'Nh√≥m thu·ªëc'
            elif 'tensanpham' in n:
                col_map2[c] = 'T√™n s·∫£n ph·∫©m'
        df2.rename(columns=col_map2, inplace=True)

        # Th√™m c√°c field chu·∫©n h√≥a ƒë·ªÉ merge
        for df_ in (df_body, df2):
            df_['active_norm'] = df_['T√™n ho·∫°t ch·∫•t'].apply(normalize_active)
            df_['conc_norm'] = df_['N·ªìng ƒë·ªô/h√†m l∆∞·ª£ng'].apply(normalize_concentration)
            df_['group_norm'] = df_['Nh√≥m thu·ªëc'].apply(normalize_group)

        # Merge d·ªØ li·ªáu, gi·ªØ 1 d√≤ng m·ªói d√≤ng g·ªëc
        merged = pd.merge(
            df_body, df2,
            on=['active_norm','conc_norm','group_norm'],
            how='left', indicator=True
        )
        merged.drop_duplicates(subset=['_orig_idx'], keep='first', inplace=True)

        # B·ªï sung ƒê·ªãa b√†n + Kh√°ch h√†ng t·ª´ file3
        hosp = df3_sel[['T√™n s·∫£n ph·∫©m','ƒê·ªãa b√†n','T√™n Kh√°ch h√†ng ph·ª• tr√°ch tri·ªÉn khai']]
        merged = pd.merge(merged, hosp, on='T√™n s·∫£n ph·∫©m', how='left')

        # Chu·∫©n b·ªã DataFrame xu·∫•t file v√† hi·ªÉn th·ªã
        export_df = merged.drop(columns=['active_norm','conc_norm','group_norm','_merge','_orig_idx'])
        display_df = merged[merged['_merge']=='both'].drop(columns=['active_norm','conc_norm','group_norm','_merge','_orig_idx'])
        st.success(f"‚úÖ T·ªïng d√≤ng kh·ªõp: {len(display_df)}")
        st.dataframe(display_df)
        st.session_state['filtered_export'] = export_df.copy()
        st.session_state['filtered_display'] = display_df.copy()

        # Tra c·ª©u ho·∫°t ch·∫•t
        kw = st.text_input("üîç Tra c·ª©u ho·∫°t ch·∫•t:")
        if kw:
            df_search = display_df[display_df['T√™n ho·∫°t ch·∫•t'].str.contains(kw, case=False, na=False)]
            st.dataframe(df_search)

        # Download k·∫øt qu·∫£
        buf = BytesIO()
        with pd.ExcelWriter(buf, engine='xlsxwriter') as writer:
            export_df.to_excel(writer, index=False, sheet_name='KetQuaLoc')
        st.download_button('‚¨áÔ∏è T·∫£i File K·∫øt Qu·∫£', data=buf.getvalue(), file_name='Ketqua_loc_all.xlsx')

# === 2. Ph√¢n T√≠ch Danh M·ª•c Th·∫ßu ===
elif option == "Ph√¢n T√≠ch Danh M·ª•c Th·∫ßu":
    st.header("üìä Ph√¢n T√≠ch Danh M·ª•c Th·∫ßu (S·ªë li·ªáu)")
    if 'filtered_display' not in st.session_state:
        st.info("Vui l√≤ng th·ª±c hi·ªán 'L·ªçc Danh M·ª•c Th·∫ßu' tr∆∞·ªõc.")
    else:
        df = st.session_state['filtered_display'].copy()
        # Chuy·ªÉn ki·ªÉu s·ªë v√† t√≠nh Tr·ªã gi√°
        df['S·ªë l∆∞·ª£ng'] = pd.to_numeric(df['S·ªë l∆∞·ª£ng'], errors='coerce').fillna(0)
        df['Gi√° k·∫ø ho·∫°ch'] = pd.to_numeric(df['Gi√° k·∫ø ho·∫°ch'], errors='coerce').fillna(0)
        df['Tr·ªã gi√°'] = df['S·ªë l∆∞·ª£ng'] * df['Gi√° k·∫ø ho·∫°ch']

        # ƒê·ªãnh d·∫°ng hi·ªÉn th·ªã s·ªë
        def fmt(x):
            if x >= 1e9:
                return f"{x/1e9:.2f} t·ª∑"
            if x >= 1e6:
                return f"{x/1e6:.2f} tri·ªáu"
            if x >= 1e3:
                return f"{x/1e3:.2f} ngh√¨n"
            return str(int(x))

        # Ch·ªçn Nh√≥m ƒëi·ªÅu tr·ªã
        groups = file4['Nh√≥m ƒëi·ªÅu tr·ªã'].dropna().unique()
        sel_group = st.selectbox("Ch·ªçn Nh√≥m ƒëi·ªÅu tr·ªã", ['(T·∫•t c·∫£)'] + list(groups))
        if sel_group != '(T·∫•t c·∫£)':
            acts = file4[file4['Nh√≥m ƒëi·ªÅu tr·ªã']==sel_group]['T√™n ho·∫°t ch·∫•t']
            df = df[df['T√™n ho·∫°t ch·∫•t'].isin(acts)]

        # T·ªïng Tr·ªã gi√° & S·ªë l∆∞·ª£ng theo Ho·∫°t ch·∫•t
        val_act = df.groupby('T√™n ho·∫°t ch·∫•t')['Tr·ªã gi√°'].sum().reset_index().sort_values('Tr·ªã gi√°', ascending=False)
        val_act['Tr·ªã gi√°'] = val_act['Tr·ªã gi√°'].apply(fmt)
        qty_act = df.groupby('T√™n ho·∫°t ch·∫•t')['S·ªë l∆∞·ª£ng'].sum().reset_index().sort_values('S·ªë l∆∞·ª£ng', ascending=False)
        qty_act['S·ªë l∆∞·ª£ng'] = qty_act['S·ªë l∆∞·ª£ng'].apply(fmt)
        st.subheader('T·ªïng Tr·ªã gi√° theo Ho·∫°t ch·∫•t')
        st.table(val_act)
        st.subheader('T·ªïng S·ªë l∆∞·ª£ng theo Ho·∫°t ch·∫•t')
        st.table(qty_act)

        # Top 10 Ti√™m/U·ªëng theo S·ªë l∆∞·ª£ng & Tr·ªã gi√°
        st.subheader('Top 10 Ho·∫°t ch·∫•t theo ƒê∆∞·ªùng d√πng')
        for route in ['ti√™m','u·ªëng']:
            sub = df[df['ƒê∆∞·ªùng d√πng'].str.contains(route, case=False, na=False)]
            top_qty = sub.groupby('T√™n ho·∫°t ch·∫•t')['S·ªë l∆∞·ª£ng'].sum().nlargest(10).reset_index()
            top_val = sub.groupby('T√™n ho·∫°t ch·∫•t')['Tr·ªã gi√°'].sum().nlargest(10).reset_index()
            top_qty['S·ªë l∆∞·ª£ng'] = top_qty['S·ªë l∆∞·ª£ng'].apply(fmt)
            top_val['Tr·ªã gi√°'] = top_val['Tr·ªã gi√°'].apply(fmt)
            st.markdown(f"**{route.capitalize()} - Top 10 theo S·ªë l∆∞·ª£ng**")
            st.table(top_qty)
            st.markdown(f"**{route.capitalize()} - Top 10 theo Tr·ªã gi√°**")
            st.table(top_val)

        # Ph√¢n t√≠ch theo Kh√°ch h√†ng ph·ª• tr√°ch
        total_sp = df['T√™n s·∫£n ph·∫©m'].nunique()
        rep = df.groupby('T√™n Kh√°ch h√†ng ph·ª• tr√°ch tri·ªÉn khai').agg(
            SL=('S·ªë l∆∞·ª£ng','sum'), TG=('Tr·ªã gi√°','sum'), SP=('T√™n s·∫£n ph·∫©m', pd.Series.nunique)
        ).reset_index()
        rep['T·ª∑ l·ªá SP'] = (rep['SP']/total_sp*100).round(2).astype(str)+'%'
        rep['SL'] = rep['SL'].apply(fmt)
        rep['TG'] = rep['TG'].apply(fmt)
        st.subheader('Ph√¢n t√≠ch theo Kh√°ch h√†ng ph·ª• tr√°ch')
        st.table(rep)

# === 3. Ph√¢n T√≠ch Danh M·ª•c Tr√∫ng Th·∫ßu ===
elif option == "Ph√¢n T√≠ch Danh M·ª•c Tr√∫ng Th·∫ßu":
    st.header("üèÜ Ph√¢n T√≠ch Danh M·ª•c Tr√∫ng Th·∫ßu")
    # TODO: Gi·ªØ nguy√™n logic hi·ªán c√≥ ho·∫∑c b·ªï sung theo y√™u c·∫ßu chi ti·∫øt
    st.info("Ch·ª©c nƒÉng n√†y ƒëang ƒë∆∞·ª£c tri·ªÉn khai ti·∫øp theo.")

# === 4. ƒê·ªÅ Xu·∫•t H∆∞·ªõng Tri·ªÉn Khai ===
elif option == "ƒê·ªÅ Xu·∫•t H∆∞·ªõng Tri·ªÉn Khai":
    st.header("üí° ƒê·ªÅ Xu·∫•t H∆∞·ªõng Tri·ªÉn Khai")
    if 'filtered_export' not in st.session_state or 'file3_temp' not in st.session_state:
        st.info("Vui l√≤ng th·ª±c hi·ªán 'L·ªçc Danh M·ª•c Th·∫ßu' tr∆∞·ªõc.")
    else:
        df_f = st.session_state['filtered_export']
        df3t = st.session_state['file3_temp']
        df3t = df3t[~df3t['ƒê·ªãa b√†n'].str.contains('T·∫°m ng∆∞ng tri·ªÉn khai|ko c√≥ ƒë·ªãa b√†n', case=False, na=False)]
        df_qty = df_f.groupby('T√™n s·∫£n ph·∫©m')['S·ªë l∆∞·ª£ng'].sum().rename('SL_tr√∫ng').reset_index()
        df_sug = pd.merge(df3t, df_qty, on='T√™n s·∫£n ph·∫©m', how='left').fillna({'SL_tr√∫ng':0})
        df_sug = pd.merge(df_sug, file4[['T√™n ho·∫°t ch·∫•t','Nh√≥m ƒëi·ªÅu tr·ªã']], on='T√™n ho·∫°t ch·∫•t', how='left')
        df_sug['S·ªë l∆∞·ª£ng ƒë·ªÅ xu·∫•t'] = (df_sug['SL_tr√∫ng']*1.5).apply(np.ceil).astype(int)
        df_sug['L√Ω do'] = df_sug.apply(
            lambda r: f"Nh√≥m {r['Nh√≥m ƒëi·ªÅu tr·ªã']} th∆∞·ªùng s·ª≠ d·ª•ng c√°c ho·∫°t ch·∫•t t∆∞∆°ng ·ª©ng; s·∫£n ph·∫©m ch√∫ng ta th·∫ø h·ªá m·ªõi, hi·ªáu qu·∫£ t·ªët h∆°n.",
            axis=1
        )
        st.subheader('File 3 t·∫°m & ƒê·ªÅ xu·∫•t tri·ªÉn khai')
        st.dataframe(df_sug)
        buf = BytesIO()
        with pd.ExcelWriter(buf, engine='xlsxwriter') as w:
            df_sug.to_excel(w, index=False, sheet_name='DeXuat')
        st.download_button('‚¨áÔ∏è T·∫£i File ƒê·ªÅ Xu·∫•t', data=buf.getvalue(), file_name='DeXuat_Thuoc.xlsx')
