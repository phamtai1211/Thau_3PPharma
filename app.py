import streamlit as st
import pandas as pd
import numpy as np
import re
import requests
import unicodedata
from io import BytesIO

# T·∫£i d·ªØ li·ªáu m·∫∑c ƒë·ªãnh t·ª´ GitHub (file2, file3, file4)
@st.cache_data
def load_default_data():
    url_file2 = "https://raw.githubusercontent.com/phamtai1211/Thau_3PPharma/main/file2.xlsx"
    url_file3 = "https://raw.githubusercontent.com/phamtai1211/Thau_3PPharma/main/file3.xlsx"
    url_file4 = "https://raw.githubusercontent.com/phamtai1211/Thau_3PPharma/main/nhom_dieu_tri.xlsx"

    file2 = pd.read_excel(BytesIO(requests.get(url_file2).content))
    file3 = pd.read_excel(BytesIO(requests.get(url_file3).content))
    file4 = pd.read_excel(BytesIO(requests.get(url_file4).content))
    return file2, file3, file4

file2, file3, file4 = load_default_data()

# Chu·∫©n h√≥a x√≥a d·∫•u
def remove_diacritics(s: str) -> str:
    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')

# Chu·∫©n h√≥a chu·ªói ƒë·ªÉ so s√°nh
def normalize_text(s: str) -> str:
    s = str(s)
    s = remove_diacritics(s).lower()
    s = re.sub(r'\s+', '', s)
    return s

# H√†m chu·∫©n h√≥a ho·∫°t ch·∫•t, h√†m l∆∞·ª£ng, nh√≥m

def normalize_active(name: str) -> str:
    return re.sub(r'\s+', ' ', re.sub(r'\(.*?\)', '', str(name))).strip().lower()

def normalize_concentration(conc: str) -> str:
    s = str(conc).lower().replace(',', '.')
    parts = [p.strip() for p in re.split(r'[;,]', s) if p.strip()]
    parts = [p for p in parts if re.search(r'\d', p)]
    if len(parts) >= 2 and re.search(r'(mg|mcg|g|%)', parts[0]) and 'ml' in parts[-1] and '/' not in parts[0]:
        return parts[0].replace(' ', '') + '/' + parts[-1].replace(' ', '')
    return ''.join([p.replace(' ', '') for p in parts])

def normalize_group(grp: str) -> str:
    return re.sub(r'\D', '', str(grp)).strip()

# Sidebar: ch·ª©c nƒÉng ch√≠nh
st.sidebar.title("Ch·ª©c nƒÉng")
option = st.sidebar.radio("Ch·ªçn ch·ª©c nƒÉng", 
    ["L·ªçc Danh M·ª•c Th·∫ßu", "Ph√¢n T√≠ch Danh M·ª•c Th·∫ßu", "Ph√¢n T√≠ch Danh M·ª•c Tr√∫ng Th·∫ßu", "ƒê·ªÅ Xu·∫•t H∆∞·ªõng Tri·ªÉn Khai"] )

# 1. L·ªçc Danh M·ª•c Th·∫ßu
if option == "L·ªçc Danh M·ª•c Th·∫ßu":
    st.header("üìÇ L·ªçc Danh M·ª•c Th·∫ßu")
    # Ti·∫øt l∆∞u v√πng ch·ªçn ƒë·ªÉ d√πng later
    regions = sorted(file3["Mi·ªÅn"].dropna().unique())
    selected_region = st.selectbox("Ch·ªçn Mi·ªÅn", ["(T·∫•t c·∫£)"] + regions)
    df3_sel = file3 if selected_region == "(T·∫•t c·∫£)" else file3[file3["Mi·ªÅn"] == selected_region]
    areas = sorted(df3_sel["V√πng"].dropna().unique())
    selected_area = st.selectbox("Ch·ªçn V√πng", ["(T·∫•t c·∫£)"] + areas) if areas else None
    if selected_area and selected_area != "(T·∫•t c·∫£)": df3_sel = df3_sel[df3_sel["V√πng"] == selected_area]
    provinces = sorted(df3_sel["T·ªânh"].dropna().unique())
    selected_prov = st.selectbox("Ch·ªçn T·ªânh", ["(T·∫•t c·∫£)"] + provinces)
    if selected_prov and selected_prov != "(T·∫•t c·∫£)": df3_sel = df3_sel[df3_sel["T·ªânh"] == selected_prov]
    hospitals = sorted(df3_sel["B·ªánh vi·ªán/SYT"].dropna().unique())
    selected_hospital = st.selectbox("Ch·ªçn B·ªánh vi·ªán/S·ªü Y T·∫ø", ["(T·∫•t c·∫£)"] + hospitals)
    if selected_hospital and selected_hospital != "(T·∫•t c·∫£)": df3_sel = df3_sel[df3_sel["B·ªánh vi·ªán/SYT"] == selected_hospital]
    # L∆∞u file3 temp
    st.session_state["file3_temp"] = df3_sel.copy()

    uploaded_file = st.file_uploader("T·∫£i l√™n file Danh M·ª•c M·ªùi Th·∫ßu (.xlsx)", type=["xlsx"])
    if uploaded_file and (selected_hospital and selected_hospital != "(T·∫•t c·∫£)"):
        xls = pd.ExcelFile(uploaded_file)
        # ch·ªçn sheet c√≥ nhi·ªÅu c·ªôt nh·∫•t
        sheet_name = max(xls.sheet_names, key=lambda name: pd.read_excel(uploaded_file, sheet_name=name, nrows=1, header=None).shape[1])
        df_raw = pd.read_excel(uploaded_file, sheet_name=sheet_name, header=None)
        # t√¨m header row (1‚Äì10) b·ªè qua merge
        header_idx = None
        score = []
        for i in range(min(10, df_raw.shape[0])):
            row = df_raw.iloc[i].fillna('').astype(str).tolist()
            text = ' '.join(row)
            norm = normalize_text(text)
            # ki·ªÉm tra ƒë·ªß c√°c tr∆∞·ªùng
            if 'tenhoatchat' in norm and ('soluong' in norm or 'nobanthe' in norm):
                header_idx = i; break
            # t√≠nh ƒëi·ªÉm
            sc = ('tenhoatchat' in norm) + ('soluong' in norm) + ('nhomthuoc' in norm) + ('nongdohamluong' in norm)
            score.append((i, sc))
        if header_idx is None:
            # l·∫•y row c√≥ ƒëi·ªÉm cao nh·∫•t n·∫øu c√≥
            idx, sc = max(score, key=lambda x: x[1])
            if sc > 0:
                header_idx = idx
                st.warning(f"T·ª± ƒë·ªông ch·ªçn d√≤ng ti√™u ƒë·ªÅ t·∫°i d√≤ng {idx+1}")
            else:
                st.error("‚ùå Kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c d√≤ng ti√™u ƒë·ªÅ trong file.")
        if header_idx is not None:
            header = df_raw.iloc[header_idx].tolist()
            df_all = df_raw.iloc[header_idx+1:].reset_index(drop=True)
            df_all.columns = header
            df_all = df_all.dropna(how='all').reset_index(drop=True)
            # chu·∫©n h√≥a t√™n c·ªôt
            col_map = {}
            for col in df_all.columns:
                n = normalize_text(col)
                if 'tenhoatchat' in n:
                    col_map[col] = 'T√™n ho·∫°t ch·∫•t'
                elif 't√™nhoatchat' in n and 'tenthanhphan' in n:
                    col_map[col] = 'T√™n ho·∫°t ch·∫•t'
                elif 'nongdo' in n or 'hamluong' in n:
                    col_map[col] = 'N·ªìng ƒë·ªô/h√†m l∆∞·ª£ng'
                elif 'nhom' in n and 'thuoc' in n:
                    col_map[col] = 'Nh√≥m thu·ªëc'
                elif 'soluong' in n:
                    col_map[col] = 'S·ªë l∆∞·ª£ng'
                elif 'gia' in n:
                    col_map[col] = 'Gi√° k·∫ø ho·∫°ch'
                elif 'duongdung' in n or 'duong' in n:
                    col_map[col] = 'ƒê∆∞·ªùng d√πng'
            df_all.rename(columns=col_map, inplace=True)

            # chu·∫©n b·ªã so s√°nh
            df_all['active_norm'] = df_all['T√™n ho·∫°t ch·∫•t'].apply(normalize_active)
            df_all['conc_norm'] = df_all['N·ªìng ƒë·ªô/h√†m l∆∞·ª£ng'].apply(normalize_concentration)
            df_all['group_norm'] = df_all['Nh√≥m thu·ªëc'].apply(normalize_group)
            df_comp = file2.copy()
            df_comp['active_norm'] = df_comp['T√™n ho·∫°t ch·∫•t'].apply(normalize_active)
            df_comp['conc_norm'] = df_comp['N·ªìng ƒë·ªô/H√†m l∆∞·ª£ng'].apply(normalize_concentration)
            df_comp['group_norm'] = df_comp['Nh√≥m thu·ªëc'].apply(normalize_group)
            # merge left ƒë·ªÉ gi·ªØ m·ªçi d√≤ng
            merged = pd.merge(df_all, df_comp, on=['active_norm','conc_norm','group_norm'], how='left', indicator=True, suffixes=('','_comp'))
            # merge v·ªõi file3 temp ƒë·ªÉ l·∫•y ƒë·ªãa b√†n, kh√°ch h√†ng
            hosp_data = file3[file3['B·ªánh vi·ªán/SYT']==selected_hospital][['T√™n s·∫£n ph·∫©m','ƒê·ªãa b√†n','T√™n Kh√°ch h√†ng ph·ª• tr√°ch tri·ªÉn khai']]
            merged = pd.merge(merged, hosp_data, on='T√™n s·∫£n ph·∫©m', how='left')
            # l∆∞u xu·∫•t
            export_df = merged.drop(columns=['active_norm','conc_norm','group_norm','_merge'])
            display_df = merged[merged['_merge']=='both'].drop(columns=['active_norm','conc_norm','group_norm','_merge'])
            st.success(f"‚úÖ T·ªïng d√≤ng kh·ªõp: {len(display_df)}")
            st.dataframe(display_df)
            # l∆∞u
            st.session_state['filtered_export'] = export_df.copy()
            st.session_state['filtered_display'] = display_df.copy()

            # cho tra c·ª©u ho·∫°t ch·∫•t
            kw = st.text_input("üîç Tra c·ª©u ho·∫°t ch·∫•t (nh·∫≠p t√™n) ƒë·ªÉ l·ªçc k·∫øt qu·∫£:")
            if kw:
                kw_norm = kw.strip().lower()
                df_search = display_df[display_df['T√™n ho·∫°t ch·∫•t'].str.lower().str.contains(kw_norm)]
                st.dataframe(df_search)

            # t·∫£i v·ªÅ file
            buf = BytesIO()
            with pd.ExcelWriter(buf, engine='xlsxwriter') as writer:
                export_df.to_excel(writer, index=False, sheet_name='KetQuaLoc')
            st.download_button('‚¨áÔ∏è T·∫£i File K·∫øt Qu·∫£', data=buf.getvalue(), file_name='Ketqua_loc_all.xlsx')
            # l∆∞u session
            st.session_state['filtered_df'] = display_df.copy()

# 2. Ph√¢n T√≠ch Danh M·ª•c Th·∫ßu
elif option == "Ph√¢n T√≠ch Danh M·ª•c Th·∫ßu":
    st.header("üìä Ph√¢n T√≠ch Danh M·ª•c Th·∫ßu (S·ªë li·ªáu)")
    if 'filtered_df' not in st.session_state:
        st.info("Vui l√≤ng th·ª±c hi·ªán 'L·ªçc Danh M·ª•c Th·∫ßu' tr∆∞·ªõc.")
    else:
        df = st.session_state['filtered_df'].copy()
        df['S·ªë l∆∞·ª£ng'] = pd.to_numeric(df['S·ªë l∆∞·ª£ng'], errors='coerce').fillna(0)
        df['Gi√° k·∫ø ho·∫°ch'] = pd.to_numeric(df.get('Gi√° k·∫ø ho·∫°ch',0), errors='coerce').fillna(0)
        df['Tr·ªã gi√°'] = df['S·ªë l∆∞·ª£ng'] * df['Gi√° k·∫ø ho·∫°ch']
        # H√†m ƒë·ªãnh d·∫°ng s·ªë
        def fmt(x):
            if x>=1e9: return f"{x/1e9:.2f} t·ª∑"
            if x>=1e6: return f"{x/1e6:.2f} tri·ªáu"
            if x>=1e3: return f"{x/1e3:.2f} ngh√¨n"
            return str(x)
        # T·ªïng tr·ªã gi√° theo ho·∫°t ch·∫•t
        val_act = df.groupby('T√™n ho·∫°t ch·∫•t')['Tr·ªã gi√°'].sum().reset_index().sort_values('Tr·ªã gi√°',ascending=False)
        val_act['Tr·ªã gi√°'] = val_act['Tr·ªã gi√°'].apply(fmt)
        st.subheader('T·ªïng Tr·ªã gi√° theo Ho·∫°t ch·∫•t')
        st.table(val_act)
        # T·ªïng s·ªë l∆∞·ª£ng theo ho·∫°t ch·∫•t
        qty_act = df.groupby('T√™n ho·∫°t ch·∫•t')['S·ªë l∆∞·ª£ng'].sum().reset_index().sort_values('S·ªë l∆∞·ª£ng',ascending=False)
        qty_act['S·ªë l∆∞·ª£ng'] = qty_act['S·ªë l∆∞·ª£ng'].apply(fmt)
        st.subheader('T·ªïng S·ªë l∆∞·ª£ng theo Ho·∫°t ch·∫•t')
        st.table(qty_act)
        # Ph√¢n t√≠ch theo ƒë∆∞·ªùng d√πng (Ti√™m & U·ªëng)
        routes = {'Ti√™m':'ti√™m','U·ªëng':'u·ªëng'}
        st.subheader('Top 10 Ho·∫°t ch·∫•t theo t·ª´ng ƒê∆∞·ªùng d√πng')
        for label, key in routes.items():
            sub = df[df['ƒê∆∞·ªùng d√πng'].str.contains(key, case=False, na=False)]
            top_qty = sub.groupby('T√™n ho·∫°t ch·∫•t')['S·ªë l∆∞·ª£ng'].sum().nlargest(10).reset_index()
            top_val = sub.groupby('T√™n ho·∫°t ch·∫•t')['Tr·ªã gi√°'].sum().nlargest(10).reset_index()
            top_val['Tr·ªã gi√°'] = top_val['Tr·ªã gi√°'].apply(fmt)
            st.markdown(f"**{label} - Top 10 theo S·ªë l∆∞·ª£ng**")
            st.table(top_qty)
            st.markdown(f"**{label} - Top 10 theo Tr·ªã gi√°**")
            st.table(top_val)
        # Ph√¢n t√≠ch theo kh√°ch h√†ng ph·ª• tr√°ch
        rep = df.groupby('T√™n Kh√°ch h√†ng ph·ª• tr√°ch tri·ªÉn khai').agg({'S·ªë l∆∞·ª£ng':'sum','Tr·ªã gi√°':'sum'}).reset_index().sort_values('Tr·ªã gi√°',ascending=False)
        rep['S·ªë l∆∞·ª£ng'] = rep['S·ªë l∆∞·ª£ng'].apply(fmt)
        rep['Tr·ªã gi√°'] = rep['Tr·ªã gi√°'].apply(fmt)
        st.subheader('Ph√¢n t√≠ch theo Kh√°ch h√†ng ph·ª• tr√°ch')
        st.table(rep)

# 3. Ph√¢n T√≠ch Danh M·ª•c Tr√∫ng Th·∫ßu
elif option == "Ph√¢n T√≠ch Danh M·ª•c Tr√∫ng Th·∫ßu":
    # (Gi·ªØ nguy√™n logic hi·ªán c√≥)
    pass

# 4. ƒê·ªÅ Xu·∫•t H∆∞·ªõng Tri·ªÉn Khai
elif option == "ƒê·ªÅ Xu·∫•t H∆∞·ªõng Tri·ªÉn Khai":
    st.header("üí° ƒê·ªÅ Xu·∫•t H∆∞·ªõng Tri·ªÉn Khai")
    if 'filtered_df' not in st.session_state or 'file3_temp' not in st.session_state:
        st.info("Vui l√≤ng th·ª±c hi·ªán 'L·ªçc' v√† 'Ph√¢n t√≠ch' tr∆∞·ªõc.")
    else:
        df_f = st.session_state['filtered_export']
        df3_temp = st.session_state['file3_temp'].copy()
        # lo·∫°i c√°c d√≤ng kh√¥ng tri·ªÉn khai
        df3_temp = df3_temp[~df3_temp['ƒê·ªãa b√†n'].str.contains('T·∫°m ng∆∞ng tri·ªÉn khai|ko c√≥ ƒë·ªãa b√†n', case=False, na=False)]
        # t√≠nh s·ªë l∆∞·ª£ng ƒë√£ tr√∫ng
        df_qty = df_f.groupby('T√™n s·∫£n ph·∫©m')['S·ªë l∆∞·ª£ng'].sum().rename('SL_trung').reset_index()
        df_sug = pd.merge(df3_temp, df_qty, on='T√™n s·∫£n ph·∫©m', how='left').fillna({'SL_trung':0})
        # ƒë·ªÅ xu·∫•t s·ªë l∆∞·ª£ng: tƒÉng 50% so v·ªõi SL_trung ƒë·ªÉ ƒë·∫°t t·ª∑ tr·ªçng >50%
        df_sug['S·ªë l∆∞·ª£ng ƒë·ªÅ xu·∫•t'] = (df_sug['SL_trung'] * 1.5).apply(np.ceil).astype(int)
        df_sug['L√Ω do'] = 'TƒÉng 50% so v·ªõi l·∫ßn tr∆∞·ªõc ƒë·ªÉ ƒë·∫°t t·ª∑ tr·ªçng >50%'
        st.subheader('File 3 t·∫°m ƒë√£ l·ªçc & ƒë·ªÅ xu·∫•t')
        st.dataframe(df_sug)
        # t·∫£i v·ªÅ
        buf = BytesIO()
        with pd.ExcelWriter(buf, engine='xlsxwriter') as w:
            df_sug.to_excel(w, index=False, sheet_name='DeXuat')
        st.download_button('‚¨áÔ∏è T·∫£i File ƒê·ªÅ Xu·∫•t', data=buf.getvalue(), file_name='DeXuat_Thuoc.xlsx')
