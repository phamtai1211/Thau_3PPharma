import streamlit as st
import pandas as pd
import numpy as np
import re
import requests
import unicodedata
import zipfile
from io import BytesIO
from openpyxl import load_workbook

# === Load default data from GitHub ===
@st.cache_data
def load_default_data():
    urls = {
        'file2': "https://raw.githubusercontent.com/phamtai1211/Thau_3PPharma/main/file2.xlsx",
        'file3': "https://raw.githubusercontent.com/phamtai1211/Thau_3PPharma/main/file3.xlsx",
        'file4': "https://raw.githubusercontent.com/phamtai1211/Thau_3PPharma/main/nhom_dieu_tri.xlsx"
    }
    data = {}
    for key, url in urls.items():
        resp = requests.get(url)
        resp.raise_for_status()
        data[key] = pd.read_excel(BytesIO(resp.content), engine='openpyxl')
    return data['file2'], data['file3'], data['file4']

file2, file3, file4 = load_default_data()

# === Text normalization helpers ===
def remove_diacritics(s: str) -> str:
    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')

def normalize_text(s: str) -> str:
    s = str(s)
    s = remove_diacritics(s).lower()
    return re.sub(r'\s+', '', s)

def normalize_active(name: str) -> str:
    return re.sub(r'\s+', ' ', re.sub(r'\(.*?\)', '', str(name))).strip().lower()

def normalize_concentration(conc: str) -> str:
    s = str(conc).lower().replace(',', '.')
    parts = [p.strip() for p in re.split(r'[;,]', s) if p.strip()]
    parts = [p for p in parts if re.search(r'\d', p)]
    if len(parts) >= 2 and re.search(r'(mg|mcg|g|%)', parts[0]) and 'ml' in parts[-1]:
        return parts[0].replace(' ', '') + '/' + parts[-1].replace(' ', '')
    return ''.join(p.replace(' ', '') for p in parts)

def normalize_group(grp: str) -> str:
    return re.sub(r'\D', '', str(grp)).strip()

# === Process uploaded Excel files ===
def process_uploaded(uploaded, df3_temp):
    # Determine sheet with most columns
    xls = pd.ExcelFile(uploaded, engine='openpyxl')
    sheet = max(xls.sheet_names, key=lambda s: pd.read_excel(uploaded, sheet_name=s, nrows=5, header=None, engine='openpyxl').shape[1])
    try:
        raw = pd.read_excel(uploaded, sheet_name=sheet, header=None, engine='openpyxl')
    except Exception:
        # Strip problematic style/theme files and dataValidations
        uploaded.seek(0)
        buf = BytesIO(uploaded.read())
        zf = zipfile.ZipFile(buf, 'r')
        out = BytesIO()
        with zipfile.ZipFile(out, 'w') as w:
            for item in zf.infolist():
                if item.filename.startswith('xl/styles') or item.filename.startswith('xl/theme'):
                    continue
                data = zf.read(item.filename)
                if item.filename.startswith('xl/worksheets/'):
                    data = re.sub(b'<dataValidations.*?</dataValidations>', b'', data, flags=re.DOTALL)
                w.writestr(item.filename, data)
        out.seek(0)
        wb = load_workbook(out, read_only=True, data_only=True)
        ws = wb[sheet]
        raw = pd.DataFrame(list(ws.iter_rows(values_only=True)))

    # Auto-detect header row among first 10
    header_idx = None
    scores = []
    for i in range(min(10, len(raw))):
        text = normalize_text(' '.join(raw.iloc[i].fillna('').astype(str).tolist()))
        sc = sum(kw in text for kw in ['tenhoatchat','soluong','nhomthuoc','nongdo'])
        scores.append((i, sc))
        if 'tenhoatchat' in text and 'soluong' in text:
            header_idx = i
            break
    if header_idx is None:
        idx, sc = max(scores, key=lambda x: x[1])
        header_idx = idx if sc > 0 else 0

    # Set header and body without user preview
    header = raw.iloc[header_idx].fillna('').astype(str).tolist()
    df_body = raw.iloc[header_idx+1:].copy()
    df_body.columns = header
    df_body = df_body.dropna(subset=header, how='all')
    df_body['_orig_idx'] = df_body.index
    df_body.reset_index(drop=True, inplace=True)

    # Map columns to standard names
    col_map = {}
    for c in df_body.columns:
        n = normalize_text(c)
        if 'tenhoatchat' in n or 'tenthanhphan' in n:
            col_map[c] = 'T√™n ho·∫°t ch·∫•t'
        elif 'nongdo' in n or 'hamluong' in n:
            col_map[c] = 'N·ªìng ƒë·ªô/h√†m l∆∞·ª£ng'
        elif 'nhom' in n and 'thuoc' in n:
            col_map[c] = 'Nh√≥m thu·ªëc'
        elif 'soluong' in n:
            col_map[c] = 'S·ªë l∆∞·ª£ng'
        elif 'duongdung' in n or 'duong' in n:
            col_map[c] = 'ƒê∆∞·ªùng d√πng'
        elif 'gia' in n:
            col_map[c] = 'Gi√° k·∫ø ho·∫°ch'
    df_body.rename(columns=col_map, inplace=True)

    # Normalize reference file2
    df2 = file2.copy()
    col_map2 = {}
    for c in df2.columns:
        n = normalize_text(c)
        if 'tenhoatchat' in n:
            col_map2[c] = 'T√™n ho·∫°t ch·∫•t'
        elif 'nongdo' in n or 'hamluong' in n:
            col_map2[c] = 'N·ªìng ƒë·ªô/h√†m l∆∞·ª£ng'
        elif 'nhom' in n and 'thuoc' in n:
            col_map2[c] = 'Nh√≥m thu·ªëc'
        elif 'tensanpham' in n:
            col_map2[c] = 'T√™n s·∫£n ph·∫©m'
    df2.rename(columns=col_map2, inplace=True)

    # Add normalized merge keys
    for df_ in (df_body, df2):
        df_['active_norm'] = df_['T√™n ho·∫°t ch·∫•t'].apply(normalize_active)
        df_['conc_norm'] = df_['N·ªìng ƒë·ªô/h√†m l∆∞·ª£ng'].apply(normalize_concentration)
        df_['group_norm'] = df_['Nh√≥m thu·ªëc'].apply(normalize_group)

    # Merge and deduplicate
    merged = pd.merge(df_body, df2, on=['active_norm','conc_norm','group_norm'], how='left', indicator=True)
    merged.drop_duplicates(subset=['_orig_idx'], keep='first', inplace=True)
    hosp = df3_temp[['T√™n s·∫£n ph·∫©m','ƒê·ªãa b√†n','T√™n Kh√°ch h√†ng ph·ª• tr√°ch tri·ªÉn khai']]
    merged = pd.merge(merged, hosp, on='T√™n s·∫£n ph·∫©m', how='left')

    # Prepare display and export DataFrames
    export_df = merged.drop(columns=['active_norm','conc_norm','group_norm','_merge','_orig_idx'])
    display_df = merged[merged['_merge']=='both'].drop(columns=['active_norm','conc_norm','group_norm','_merge','_orig_idx'])
    return display_df, export_df

# === Main UI ===
st.sidebar.title("Ch·ª©c nƒÉng")
option = st.sidebar.radio("Ch·ªçn ch·ª©c nƒÉng", [
    "L·ªçc Danh M·ª•c Th·∫ßu",
    "Ph√¢n T√≠ch Danh M·ª•c Th·∫ßu",
    "Ph√¢n T√≠ch Danh M·ª•c Tr√∫ng Th·∫ßu",
    "ƒê·ªÅ Xu·∫•t H∆∞·ªõng Tri·ªÉn Khai"
])

# === 1. L·ªçc Danh M·ª•c Th·∫ßu ===
if option == "L·ªçc Danh M·ª•c Th·∫ßu":
    st.header("üìÇ L·ªçc Danh M·ª•c Th·∫ßu")
    df3_temp = file3.copy()
    for col in ['Mi·ªÅn','V√πng','T·ªânh','B·ªánh vi·ªán/SYT']:
        opts = ['(T·∫•t c·∫£)'] + sorted(df3_temp[col].dropna().unique())
        sel = st.selectbox(f"Ch·ªçn {col}", opts)
        if sel != '(T·∫•t c·∫£)':
            df3_temp = df3_temp[df3_temp[col] == sel]

    uploaded = st.file_uploader("T·∫£i l√™n file Danh M·ª•c M·ªùi Th·∫ßu (.xlsx)", type=['xlsx','xls'])
    if uploaded:
        display_df, export_df = process_uploaded(uploaded, df3_temp)
        st.success(f"‚úÖ T·ªïng d√≤ng kh·ªõp: {len(display_df)}")
        st.write(display_df.fillna('').astype(str))

        # l∆∞u session
        st.session_state['filtered_display'] = display_df.copy()
        st.session_state['filtered_export']  = export_df.copy()
        st.session_state['file3_temp']       = df3_temp.copy()

        # fix NameError & t√≠nh c·ªôt Tr·ªã gi√°
        df = display_df.copy()
        df['S·ªë l∆∞·ª£ng']     = pd.to_numeric(df['S·ªë l∆∞·ª£ng'], errors='coerce').fillna(0)
        df['Gi√° k·∫ø ho·∫°ch'] = pd.to_numeric(df.get('Gi√° k·∫ø ho·∫°ch', 0), errors='coerce').fillna(0)
        df['Tr·ªã gi√°']      = df['S·ªë l∆∞·ª£ng'] * df['Gi√° k·∫ø ho·∫°ch']

        # (c√°c b·∫£ng summary n·∫øu c√≥, v√≠ d·ª• T·ªïng Tr·ªã gi√° theo Ho·∫°t ch·∫•t)
        # ‚Ä¶

        # n√∫t download k·∫øt qu·∫£ l·ªçc
        from io import BytesIO
        buf = BytesIO()
        with pd.ExcelWriter(buf, engine='xlsxwriter') as writer:
            st.session_state['filtered_export'].to_excel(
                writer, index=False, sheet_name='DanhMucLoc'
            )
        buf.seek(0)
        st.download_button(
            label='‚¨áÔ∏è T·∫£i file Danh M·ª•c L·ªçc (.xlsx)',
            data=buf.getvalue(),
            file_name='DanhMucLoc.xlsx',
            mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        )
# 2. Ph√¢n T√≠ch Danh M·ª•c Th·∫ßu
elif option == "Ph√¢n T√≠ch Danh M·ª•c Th·∫ßu":
    st.header("üìä Ph√¢n T√≠ch Danh M·ª•c Th·∫ßu")
    # 2.1. L·∫•y data ƒë√£ l·ªçc
    df = st.session_state.get('filtered_display', pd.DataFrame()).copy()
    if df.empty:
        st.warning("B·∫°n ch∆∞a th·ª±c hi·ªán l·ªçc danh m·ª•c. Vui l√≤ng v√†o tab 'L·ªçc Danh M·ª•c Th·∫ßu' tr∆∞·ªõc.")
    else:
        # 2.2. ƒê·ªãnh nghƒ©a h√†m format s·ªë
        def fmt(x):
            if x >= 1e9: return f"{x/1e9:.2f} t·ª∑"
            if x >= 1e6: return f"{x/1e6:.2f} tri·ªáu"
            if x >= 1e3: return f"{x/1e3:.2f} ngh√¨n"
            return str(int(x))
        
        # 2.3. Ch·ªçn Nh√≥m ƒëi·ªÅu tr·ªã
        groups = file4['Nh√≥m ƒëi·ªÅu tr·ªã'].dropna().unique().tolist()
        sel_g = st.selectbox("Ch·ªçn Nh√≥m ƒëi·ªÅu tr·ªã", ['(T·∫•t c·∫£)'] + groups)
        if sel_g != '(T·∫•t c·∫£)':
            acts = file4[file4['Nh√≥m ƒëi·ªÅu tr·ªã'] == sel_g]['T√™n ho·∫°t ch·∫•t']
            df = df[df['T√™n ho·∫°t ch·∫•t'].isin(acts)]

        # 2.4. T√≠nh v√† hi·ªÉn th·ªã T·ªïng Tr·ªã gi√° theo Ho·∫°t ch·∫•t
        val = (
            df
            .groupby('T√™n ho·∫°t ch·∫•t')['Tr·ªã gi√°']
            .sum()
            .reset_index()
            .sort_values('Tr·ªã gi√°', ascending=False)
        )
        val['Tr·ªã gi√°'] = val['Tr·ªã gi√°'].apply(fmt)
        st.subheader("T·ªïng Tr·ªã gi√° theo Ho·∫°t ch·∫•t")
        st.table(val)

        # 2.5. T√≠nh v√† hi·ªÉn th·ªã T·ª∑ tr·ªçng s·ªë l∆∞·ª£ng theo Ho·∫°t ch·∫•t
        qty = (
            df
            .groupby('T√™n ho·∫°t ch·∫•t')['S·ªë l∆∞·ª£ng']
            .sum()
            .reset_index()
            .sort_values('S·ªë l∆∞·ª£ng', ascending=False)
        )
        total_qty = qty['S·ªë l∆∞·ª£ng'].sum()
        qty['T·ª∑ tr·ªçng'] = qty['S·ªë l∆∞·ª£ng'].apply(lambda x: f"{x/total_qty:.2%}")
        st.subheader("T·ª∑ tr·ªçng s·ªë l∆∞·ª£ng theo Ho·∫°t ch·∫•t")
        st.table(qty)

        # 2.6. Hi·ªÉn th·ªã t·ªïng s·ªë li·ªáu ch√≠nh
        st.subheader("Ch·ªâ s·ªë t·ªïng quan")
        st.metric("T·ªïng Tr·ªã gi√°", fmt(df['Tr·ªã gi√°'].sum()))
        st.metric("T·ªïng S·ªë l∆∞·ª£ng", int(df['S·ªë l∆∞·ª£ng'].sum()))
        ```

**H∆∞·ªõng d·∫´n**  
1. X√≥a kh·ªëi c≈© `elif option == "Ph√¢n T√≠ch Danh M·ª•c Th·∫ßu":` ƒë·∫øn d√≤ng tr∆∞·ªõc `elif option == "ƒê·ªÅ Xu·∫•t":`.  
2. D√°n ƒëo·∫°n tr√™n v·ªõi **indent 4 spaces**.  
3. Ch·∫°y l·∫°i app, ph·∫ßn ph√¢n t√≠ch s·∫Ω hi·ªÉn th·ªã dropdown ‚ÄúCh·ªçn Nh√≥m ƒëi·ªÅu tr·ªã‚Äù ƒë√∫ng ch·ªó v√† c√°c b·∫£ng b√°o c√°o.


# 3. Ph√¢n T√≠ch Danh M·ª•c Tr√∫ng Th·∫ßu
elif option == "Ph√¢n T√≠ch Danh M·ª•c Tr√∫ng Th·∫ßu":
    st.header("üèÜ Ph√¢n T√≠ch Danh M·ª•c Tr√∫ng Th·∫ßu")
    st.info("Ch·ª©c nƒÉng ƒëang x√¢y d·ª±ng...")

# 4. ƒê·ªÅ Xu·∫•t H∆∞·ªõng Tri·ªÉn Khai
elif option == "ƒê·ªÅ Xu·∫•t H∆∞·ªõng Tri·ªÉn Khai":
    st.header("üí° ƒê·ªÅ Xu·∫•t H∆∞·ªõng Tri·ªÉn Khai")
    if 'filtered_export' not in st.session_state or 'file3_temp' not in st.session_state:
        st.info("Vui l√≤ng th·ª±c hi·ªán 'L·ªçc Danh M·ª•c Th·∫ßu' tr∆∞·ªõc.")
    else:
        df_f = st.session_state['filtered_export']
        df3t = st.session_state['file3_temp']
        df3t = df3t[~df3t['ƒê·ªãa b√†n'].str.contains('T·∫°m ng∆∞ng tri·ªÉn khai|ko c√≥ ƒë·ªãa b√†n', case=False, na=False)]
        qty = df_f.groupby('T√™n s·∫£n ph·∫©m')['S·ªë l∆∞·ª£ng'].sum().rename('SL_tr√∫ng').reset_index()
        sug = pd.merge(df3t, qty, on='T√™n s·∫£n ph·∫©m', how='left').fillna({'SL_tr√∫ng':0})
        sug = pd.merge(sug, file4[['T√™n ho·∫°t ch·∫•t','Nh√≥m ƒëi·ªÅu tr·ªã']], on='T√™n ho·∫°t ch·∫•t', how='left')
        sug['S·ªë l∆∞·ª£ng ƒë·ªÅ xu·∫•t'] = (sug['SL_tr√∫ng']*1.5).apply(np.ceil).astype(int)
        sug['L√Ω do'] = sug.apply(lambda r: f"Nh√≥m {r['Nh√≥m ƒëi·ªÅu tr·ªã']} th∆∞·ªùng s·ª≠ d·ª•ng; s·∫£n ph·∫©m m·ªõi, hi·ªáu qu·∫£ t·ªët h∆°n.", axis=1)
        # display with fallback
        try:
            st.dataframe(sug)
        except ValueError:
            st.table(sug)
        buf = BytesIO()
        with pd.ExcelWriter(buf, engine='xlsxwriter') as w:
            sug.to_excel(w, index=False, sheet_name='ƒê·ªÅ Xu·∫•t')
        st.download_button('‚¨áÔ∏è T·∫£i ƒê·ªÅ Xu·∫•t', data=buf.getvalue(), file_name='DeXuat.xlsx')
